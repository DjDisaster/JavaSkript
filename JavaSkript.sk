# Attempt 2 at a transpiler.
# this time after actually doing research

# Basic method for a token with a VALUE and TYPE.
function newToken(tokenType: String, tokenValue: object) :: nbt compound:
	set {_nbt} to nbt from "{}"
	set tag "value" of {_nbt} to {_tokenValue} if {_tokenValue} != null else "NULL"
	set tag "type" of {_nbt} to {_tokenType}
	return {_nbt}

# Basic Getters for Tokens.
function getType(nbt: nbt compound) :: string:
	return tag "type" of {_nbt}
function getValue(nbt: nbt compound) :: object:
	return tag "value" of {_nbt}
	
# This functions merges literals if the last 2 values are literals.
function mergeLiteral(n: nbt compounds) :: nbt compounds:
	set {_size} to size of {_n::*}
	set {_lastValue} to {_n::%{_size}%}
	set {_prevValue} to {_n::%{_size} - 1%}
	
	# Checks if the newest and the second newest value are both literals
	if getType({_lastValue}), getType({_prevValue}) = "Literal":
		# Appends newest value to one before and deletes it.
		set {_newValue} to join getValue({_prevValue}), getValue({_lastValue})
		set tag "value" of {_n::%{_Size} - 1%} to {_newValue}
		delete {_n::%{_Size}%}
	return {_n::*}
	
	
# The function for tokenizing a string, joins them first.
# Accepts a list.
function tokenize(s: strings):
	# Going to put everything into a single string for ease
	set {_s} to join (join {_s::*} with ";"), ";"
	
	# Get the length and set the current point.
	set {_pointer} to 0
	set {_sLength} to {_s}.length()
	
	# Loop each character in the string.
	# Tokens are stored in {_tokens::*}
	while {_pointer} < {_sLength}:
		set {_c} to "%{_s}.charAt({_pointer}).toString()%"
		
		# Checks for A-Z characters.
		if {_c}.matches("[a-z]") = true:
			set {_v} to ""
			while {_s}.charAt({_pointer}).toString().matches("[a-z]") = true:
				set {_v} to join {_v}, "%{_s}.charAt({_pointer})%"
				add 1 to {_pointer}
			remove 1 from {_pointer}
			if {_v} = "Return":
				add newToken("Return", null) to {_tokens::*}
			else if {_v} = "Function":
				add newToken("Function", null) to {_tokens::*}
			else:
				# Its probably a literal.
				add newToken("Literal", {_v}) to {_tokens::*}
				# If the last token was also a literal, we may aswell merge them. (BELOW FUNCTION)
				set {_tokens::*} to mergeLiteral({_tokens::*})
		# Checks for a semi-colon.
		else if {_c}.matches(";") = true:
			add newToken("SemiColon", null) to {_tokens::*}
		# Checks for a number 0-9.
		else if {_c}.matches("[0-9]") = true:
			set {_v} to ""
			while {_s}.charAt({_pointer}).toString().matches("[0-9]") = true:
				set {_v} to join {_v}, "%{_s}.charAt({_pointer})%"
				add 1 to {_pointer}
			remove 1 from {_pointer}
			add newToken("Number", {_v}) to {_tokens::*}
		# Checks for an indent/tab.
		else if {_c}.equals("	") = true:
			while {_s}.charAt({_pointer}).toString().equals("	") = true:
				add newToken("Indentation", null) to {_tokens::*}
				add 1 to {_pointer}
			remove 1 from {_pointer}
		# Checks for anything else and assumes literal.
		else:
			# Its probably a literal. Likely a colon or a space.
			add newToken("Literal", {_c}) to {_tokens::*}
			# If the last token was also a literal, we may aswell merge them. (BELOW FUNCTION)
			set {_tokens::*} to mergeLiteral({_tokens::*})

		add 1 to {_n}
		if {_n} >= 1000:
			broadcast "N >= 1000"
			stop
	
		add 1 to {_pointer}
		
	broadcast "%{_tokens::*}%"
		
on load:
	tokenize("function test():", "	return 0")
